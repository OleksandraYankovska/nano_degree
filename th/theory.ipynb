{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0667ba-3e8a-4267-9778-cc568db9056c",
   "metadata": {},
   "source": [
    "What does “Data Cleansing” mean? What are the best ways to practice this?\n",
    "What is the difference between data profiling and data mining?\n",
    "Define Outlier with an example.\n",
    "What is “Collaborative Filtering”?\n",
    "What is “Time Series Analysis”?\n",
    "Explain the core steps of a Data Analysis project?\n",
    "What are the characteristics of a good data model?\n",
    "Explain and provide examples of univariate, bivariate, and multivariate analysis?\n",
    "What is a Linear Regression?\n",
    "In terms of modelling data, what do we mean by Over-fitting and Under-fitting?\n",
    "\n",
    "\n",
    "\n",
    "Data cleansing, also referred to as data cleaning or data scrubbing, is the process of fixing incorrect, incomplete, duplicate or otherwise erroneous data in a data set. It involves identifying data errors and then changing, updating or removing data to correct them. Data cleansing improves data quality and helps provide more accurate, consistent and reliable information for decision-making in an organization. \n",
    "Data cleansing is a key part of the overall data management process and one of the core components of data preparation work that readies data sets for use in business intelligence (BI) and data science applications. It's typically done by data quality analysts and engineers or other data management professionals. But data scientists, BI analysts and business users may also clean data or take part in the data cleansing process for their own applications.\n",
    "Data cleansing, data cleaning and data scrubbing are often used interchangeably. For the most part, they're considered to be the same thing. In some cases, though, data scrubbing is viewed as an element of data cleansing that specifically involves removing duplicate, bad, unneeded or old data from data sets.  \n",
    "Data scrubbing also has a different meaning in connection with data storage. In that context, it's an automated function that checks disk drives and storage systems to make sure the data they contain can be read and to identify any bad sectors or blocks.\n",
    "Data cleansing addresses a range of errors and issues in data sets, including inaccurate, invalid, incompatible and corrupt data. Some of those problems are caused by human error during the data entry process, while others result from the use of different data structures, formats and terminology in separate systems throughout an organization.\n",
    "The types of issues that are commonly fixed as part of data cleansing projects include the following:\n",
    "Typos and invalid or missing data. Data cleansing corrects various structural errors in data sets. For example, that includes misspellings and other typographical errors, wrong numerical entries, syntax errors and missing values, such as blank or null fields that should contain data.\n",
    "Inconsistent data. Names, addresses and other attributes are often formatted differently from system to system. For example, one data set might include a customer's middle initial, while another doesn't. Data elements such as terms and identifiers may also vary. Data cleansing helps ensure that data is consistent so it can be analyzed accurately.\n",
    "Duplicate data. Data cleansing identifies duplicate records in data sets and either removes or merges them through the use of deduplication measures. For example, when data from two systems is combined,duplicate data entries can be reconciled to create single records.\n",
    "Irrelevant data. Some data -- outliers or out-of-date entries, for example -- may not be relevant to analytics applications and could skew their results. Data cleansing removes redundant data from data sets, which streamlines data preparation and reduces the required amount of data processing and storage resources.\n",
    "The scope of data cleansing work varies depending on the data set and analytics requirements. For example, a data scientist doing fraud detection analysis on credit card transaction data may want to retain outlier values because they could be a sign of fraudulent purchases. But the data scrubbing process typically includes the following actions:\n",
    "Inspection and profiling. First, data is inspected and audited to assess its quality level and identify issues that need to be fixed. This step usually involves data profiling, which documents relationships between data elements, checks data quality and gathers statistics on data sets to help find errors, discrepancies and other problems.\n",
    "Cleaning. This is the heart of the cleansing process, when data errors are corrected and inconsistent, duplicate and redundant data is addressed.\n",
    "Verification. After the cleaning step is completed, the person or team that did the work should inspect the data again to verify its cleanliness and make sure it conforms to internal data quality rules and standards.\n",
    "Reporting. The results of the data cleansing work should then be reported to IT and business executives to highlight data quality trends and progress. The report could include the number of issues found and corrected, plus updated metrics on the data's quality levels.\n",
    "The cleansed data can then be moved into the remaining stages of data preparation, starting with data structuring and data transformation, to continue readying it for analytics uses. \n",
    "Data cleansing provides the following business and data management benefits:\n",
    "Improved decision-making. With more accurate data, analytics applications can produce better results. That enables organizations to make more informed decisions on business strategies and operations, as well as things like patient care and government programs.\n",
    "More effective marketing and sales. Customer data is often wrong, inconsistent or out of date. Cleaning up the data in customer relationship management and sales systems helps improve the effectiveness of marketing campaigns and sales efforts.\n",
    "Better operational performance. Clean, high-quality data helps organizations avoid inventory shortages, delivery snafus and other business problems that can result in higher costs, lower revenues and damaged relationships with customers.\n",
    "Increased use of data. Data has become a key corporate asset, but it can't generate business value if it isn't used. By making data more trustworthy, data cleansing helps convince business managers and workers to rely on it as part of their jobs.\n",
    "Reduced data costs. Data cleansing stops data errors and issues from further propagating in systems and analytics applications. In the long term, that saves time and money, because IT and data management teams don't have to continue fixing the same errors in data sets.\n",
    "2. Data profiling is also known as data archaeology. It is a process of evaluating data from an existing source and analyzing and summarizing useful information about that data. The primary task of data profiling is to identify issues like incorrect values, anomalies, and missing values in the initial phases of data analysis. It can be done for many reasons, but the most common part of data profiling is to find the quality of data as a component of a huge project. Data profiling is linked with ETL (Extract, Transform, and Load) process to transfer data from one system to another.\n",
    "There are three different techniques of data profiling\n",
    "Structure discovery\n",
    "Content discovery\n",
    "Relationship discovery\n",
    "Data profiling can be performed in various ways; these are some methods that can be used.\n",
    "Cross Profiling. It counts how many times every value appears within each column in a table. It helps to discover the trends and patterns within the data\n",
    "Cross column. The primary purpose of this method is to look across the column to perform key and dependency analysis. Key analysis scans the total values in a table to place a potential primary key. Dependency analysis finds the relationships within the sets of data. Both these analyses find the relationships and dependencies within a table.\n",
    "\n",
    "Cross table profiling. Cross table profiling looks across tables to identify the potential foreign keys. It helps find the differences and similarities in syntax and data types between tables to determine which data might be redundant and which could be mapped together.\n",
    "Data mining refers to a process used by various organizations to transform raw data into useful information. Many organizations use software to discover data, trends, and patterns in a huge amount of data to understand more about customer behaviours and develop better marketing strategies. Data mining has broad applications in various fields, like the IT sector and science and technology. Data mining is also known as KDD (Knowledge Discovery in Data).\n",
    "These are the given steps involved in the process of data mining\n",
    "Business Understanding:It involves understanding every aspect of the product, and employees do their work accordingly.\n",
    "Data Selection: It involves data selection. Data selection means selecting the best data set from where we can discover and extract data.\n",
    "Data Preparation:In this step, the gathered information is used for the further process.\n",
    "Modelling:In the modelling process, we reconstruct the given data as per the user requirements.\n",
    "Evaluation:Evaluation is one of the most important processes of data mining. It covers every aspect of the process to analyze for a possible fault in the process.\n",
    "Deployment: Once everything is checked, the data is ready to be deployed and used for the next process.\n",
    "Difference\n",
    "\n",
    "Data Profiling is a process of evaluating data from an existing source and analyzing and summarizing useful information about that data.\t\n",
    "Data mining refers to a process of analyzing the gathered information and collecting insights and statistics about the data.\n",
    "\n",
    "It is also called data archaeology.\t\n",
    "It is also known as KDD (Knowledge Discovery in Databases).\n",
    "\n",
    "It is executed on structured as well as unstructured data.\t\n",
    "Generally, it is executed on the structured data.\n",
    "\n",
    "It extracts the data from the existing raw data.\t\n",
    "The data extraction process involves some computer-based methodologies and some algorithms.\n",
    "\n",
    "It involves the discovery and analytical techniques to collect useful information related to the data.\t\n",
    "It involves various techniques to perform tasks, such as classification, clustering, regression, association rule and neural network.\n",
    "\n",
    "The tools used for data profiling are Microsoft Docs, IBM Information Analyzer, Melisa Data Profiler, etc.\t\n",
    "The tools used for data mining are Orange, RapidMiner, SPSS, Rattle, Sisense, Weka, etc.\n",
    "\n",
    "\n",
    "3. \n",
    "An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.\n",
    "\n",
    "The box plot is a useful graphical display for describing the behavior of the data in the middle as well as at the ends of the distributions. The box plot uses the median and the lower and upper quartiles (defined as the 25th and 75th percentiles). If the lower quartile is Q1 and the upper quartile is Q3, then the difference (Q3 - Q1) is called the interquartile range or IQ.\n",
    "\n",
    "A box plot is constructed by drawing a box between the upper and lower quartiles with a solid line drawn across the box to locate the median. \n",
    "The following quantities (called fences) are needed for identifying extreme values in the tails of the distribution:\n",
    "lower inner fence: Q1 - 1.5*IQ\n",
    "upper inner fence: Q3 + 1.5*IQ\n",
    "lower outer fence: Q1 - 3*IQ\n",
    "upper outer fence: Q3 + 3*IQ\n",
    "Outlier detection criteria\t\n",
    "A point beyond an inner fence on either side is considered a mild outlier. A point beyond an outer fence is considered an extreme outlier.\n",
    "\n",
    "Example of an outlier box plot\t\n",
    "The data set of N = 90 ordered observations as shown below is examined for outliers:\n",
    "30, 171, 184, 201, 212, 250, 265, 270, 272, 289, 305, 306, 322, 322, 336, 346, 351, 370, 390, 404, 409, 411, 436, 437, 439, 441, 444, 448, 451, 453, 470, 480, 482, 487, 494, 495, 499, 503, 514, 521, 522, 527, 548, 550, 559, 560, 570, 572, 574, 578, 585, 592, 592, 607, 616, 618, 621, 629, 637, 638, 640, 656, 668, 707, 709, 719, 737, 739, 752, 758, 766, 792, 792, 794, 802, 818, 830, 832, 843, 858, 860, 869, 918, 925, 953, 991, 1000, 1005, 1068, 1441\n",
    "\n",
    "The above data is available as a text file.\n",
    "\n",
    "The computations are as follows:\n",
    "\n",
    "Median = (n+1)/2 largest data point = the average of the 45th and 46th ordered points = (559 + 560)/2 = 559.5\n",
    "Lower quartile = .25(N+1)th ordered point = 22.75th ordered point = 411 + .75(436-411) = 429.75\n",
    "Upper quartile = .75(N+1)th ordered point = 68.25th ordered point = 739 +.25(752-739) = 742.25\n",
    "Interquartile range = 742.25 - 429.75 = 312.5\n",
    "Lower inner fence = 429.75 - 1.5 (312.5) = -39.0\n",
    "Upper inner fence = 742.25 + 1.5 (312.5) = 1211.0\n",
    "Lower outer fence = 429.75 - 3.0 (312.5) = -507.75\n",
    "Upper outer fence = 742.25 + 3.0 (312.5) = 1679.75\n",
    "From an examination of the fence points and the data, one point (1441) exceeds the upper inner fence and stands out as a mild outlier; there are no extreme outliers.\n",
    "\n",
    "\n",
    "4. \n",
    "Collaborative filtering filters information by using the interactions and data collected by the system from other users. It’s based on the idea that people who agreed in their evaluation of certain items are likely to agree again in the future.\n",
    "\n",
    "The concept is simple: when we want to find a new movie to watch we’ll often ask our friends for recommendations. Naturally, we have greater trust in the recommendations from friends who share tastes similar to our own.\n",
    "\n",
    "Most collaborative filtering systems apply the so-called similarity index-based technique. In the neighborhood-based approach, a number of users are selected based on their similarity to the active user. Inference for the active user is made by calculating a weighted average of the ratings of the selected users.\n",
    "\n",
    "Collaborative-filtering systems focus on the relationship between users and items. The similarity of items is determined by the similarity of the ratings of those items by the users who have rated both items.\n",
    "\n",
    "There are two classes of Collaborative Filtering: \n",
    "\n",
    "User-based, which measures the similarity between target users and other users.\n",
    "Item-based, which measures the similarity between the items that target users rate or interact with and other items.\n",
    "\n",
    "\n",
    "5.\n",
    "Time series analysis is a specific way of analyzing a sequence of data points collected over an interval of time. In time series analysis, analysts record data points at consistent intervals over a set period of time rather than just recording the data points intermittently or randomly. However, this type of analysis is not merely the act of collecting data over time. \n",
    "\n",
    "What sets time series data apart from other data is that the analysis can show how variables change over time. In other words, time is a crucial variable because it shows how the data adjusts over the course of the data points as well as the final results. It provides an additional source of information and a set order of dependencies between the data. \n",
    "\n",
    "Time series analysis typically requires a large number of data points to ensure consistency and reliability. An extensive data set ensures you have a representative sample size and that analysis can cut through noisy data. It also ensures that any trends or patterns discovered are not outliers and can account for seasonal variance. Additionally, time series data can be used for forecasting—predicting future data based on historical data.\n",
    "\n",
    "Time series analysis is used for non-stationary data—things that are constantly fluctuating over time or are affected by time. Industries like finance, retail, and economics frequently use time series analysis because currency and sales are always changing. Stock market analysis is an excellent example of time series analysis in action, especially with automated trading algorithms. Likewise, time series analysis is ideal for forecasting weather changes, helping meteorologists predict everything from tomorrow’s weather report to future years of climate change. Examples of time series analysis in action include:\n",
    "\n",
    "Weather data\n",
    "Rainfall measurements\n",
    "Temperature readings\n",
    "Heart rate monitoring (EKG)\n",
    "Brain monitoring (EEG)\n",
    "Quarterly sales\n",
    "Stock prices\n",
    "Automated stock trading\n",
    "Industry forecasts\n",
    "Interest rates\n",
    "\n",
    "6. \n",
    "1. Understand the Business Issues\n",
    "When presented with a data project, you will be given a brief outline of the expectations. From that outline, you should identify the key objectives that the business is trying to uncover. You should examine the overall scope of the work, business objectives, information the stakeholders are seeking, the type of analysis they want you to use, and the deliverables (the outputs of the project) they want.\n",
    "You need to have these elements clearly defined prior to beginning your data analysis project to provide the best deliverable you can. Additionally, it’s important to ask as many questions as you can at the outset of the project because, often, you may not have another chance before the completion of the project.\n",
    "2. Understand Your Data Set\n",
    "There are a variety of tools you can use to organize your data. When presented with a small dataset, you can use Excel, but for heftier jobs, you’ll likely want to use more rigid tools to explore and prepare your data. Muñoz suggests R, Python, Alteryx, Tableau Prep or Tableau Desktop to help prepare your data for it’s cleaning.\n",
    "Within these programs, you should identify key variables to help categorize the data. When going through the data sets, look for errors in the data. These can be anything from omitted data, data that doesn’t logically make sense, duplicate data, or even spelling errors. These missing variables need to be amended so you can properly clean your data.\n",
    "3. Prepare the Data\n",
    "Once you have organized and identified all the variables in your dataset, you can begin cleaning. In this step, you will input missing variables, create new broad categories to help categorize data that doesn’t have a proper place, and remove any duplicates in your data. Imputing average data scores for categories where there are missing values will help the data be processed more efficiently without skewing it.\n",
    "4. Perform Exploratory Analysis and Modeling\n",
    "In this step, you will begin building models to test your data and seek out answers to the objectives given. Using different statistical modeling methods, you can determine which is the best for your data. Common models include linear regressions, decision trees, and random forest modeling, among others.\n",
    "5. Validate Your Data\n",
    "Once you have crafted your models, you’ll need to assess the data and determine if you have the correct information for your deliverable. Did the models work properly? Does the data need more cleaning? Did you find the outcome the client was looking to answer? If not, you may need to go over the previous steps again. You should expect a lot of trial and error!\n",
    "6. Visualize and Present Your Findings\n",
    "Once you have all your deliverables met, you can begin your data visualization. In many cases, data visualization will be crucial in communicating your findings to the client. Not all clients are data-savvy, and interactive visualization tools like Tableau are tremendously useful in illustrating your conclusions to clients. Being able to tell a story with your data is essential. Telling a story will help explain to the client the value of your findings.\n",
    "As with any project, you need to identify your objectives clearly. Outlining your work will ensure you get the best deliverables for your clients. While all of these steps are important, if you start the project without all the data you need, you are likely to have to backtrack.\n",
    "\n",
    "7. \n",
    "What are the characteristics of a good data model?\n",
    "There is no one definitive answer to this question as it depends on the specific application and context in which the data model will be used. However, in general, a good data model should be designed in such a way as to support the efficient and effective retrieval of data, while also being flexible enough to accommodate future changes. Additionally, the data model should be easy to understand and use, in order to avoid errors and inconsistencies.\n",
    "A physical data model, on the other hand, is a database-specific model that represents relational data objects (for example, tables, columns, primary and foreign keys). A good model can be used to generate DDL statements, which can then be deployed to a database server. When selecting data models, it is critical to keep in mind which model will lead to simpler code at the application level. A data model is defined as the logical division of data elements and the flow of data between them. A hierarchical data model can represent one to many relationships in a tree-like format. Data models are fundamental components of software development and analytic integration. They define relationships between data elements in a database and outline how they can be used.\n",
    "\n",
    "8.\n",
    "Univariate data –\n",
    "This type of data consists of only one variable. The analysis of univariate data is thus the simplest form of analysis since the information deals with only one quantity that changes. It does not deal with causes or relationships and the main purpose of the analysis is to describe the data and find patterns that exist within it. The example of a univariate data can be height.\n",
    "Suppose that the heights of seven students of a class is recorded(figure 1),there is only one variable that is height and it is not dealing with any cause or relationship. The description of patterns found in this type of data can be made by drawing conclusions using central tendency measures (mean, median and mode), dispersion or spread of data (range, minimum, maximum, quartiles, variance and standard deviation) and by using frequency distribution tables, histograms, pie charts, frequency polygon and bar charts.\n",
    "\n",
    "Bivariate data –\n",
    "This type of data involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to find out the relationship among the two variables.Example of bivariate data can be temperature and ice cream sales in summer season.\n",
    "Suppose the temperature and ice cream sales are the two variables of a bivariate data(figure 2). Here, the relationship is visible from the table that temperature and sales are directly proportional to each other and thus related because as the temperature increases, the sales also increase. Thus bivariate data analysis involves comparisons, relationships, causes and explanations. These variables are often plotted on X and Y axis on the graph for better understanding of data and one of these variables is independent while the other is dependent.\n",
    "\n",
    "Multivariate data –\n",
    "When the data involves three or more variables, it is categorized under multivariate. Example of this type of data is suppose an advertiser wants to compare the popularity of four advertisements on a website, then their click rates could be measured for both men and women and relationships between variables can then be examined.\n",
    "It is similar to bivariate but contains more than one dependent variable. The ways to perform analysis on this data depends on the goals to be achieved.Some of the techniques are regression analysis,path analysis,factor analysis and multivariate analysis of variance (MANOVA).\n",
    "\n",
    "\n",
    "\n",
    "9. Linear regression is a basic and commonly used type of predictive analysis.  The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable?  (2) Which variables in particular are significant predictors of the outcome variable, and in what way do they–indicated by the magnitude and sign of the beta estimates–impact the outcome variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables.  The simplest form of the regression equation with one dependent and one independent variable is defined by the formula y = c + b*x, where y = estimated dependent variable score, c = constant, b = regression coefficient, and x = score on the independent variable.\n",
    "Naming the Variables.  There are many names for a regression’s dependent variable.  It may be called an outcome variable, criterion variable, endogenous variable, or regressand.  The independent variables can be called exogenous variables, predictor variables, or regressors.\n",
    "Three major uses for regression analysis are (1) determining the strength of predictors, (2) forecasting an effect, and (3) trend forecasting.\n",
    "First, the regression might be used to identify the strength of the effect that the independent variable(s) have on a dependent variable.  Typical questions are what is the strength of relationship between dose and effect, sales and marketing spending, or age and income.\n",
    "Second, it can be used to forecast effects or impact of changes.  That is, the regression analysis helps us to understand how much the dependent variable changes with a change in one or more independent variables.  A typical question is, “how much additional sales income do I get for each additional $1000 spent on marketing?”\n",
    "Third, regression analysis predicts trends and future values.  The regression analysis can be used to get point estimates.  A typical question is, “what will the price of gold be in 6 months?”\n",
    "\n",
    "\n",
    "\n",
    "10. Underfitting is a situation when your model is too simple for your data. More formally, your hypothesis about data distribution is wrong and too simple — for example, your data is quadratic and your model is linear. This situation is also called high bias. This means that your algorithm can do accurate predictions, but the initial assumption about the data is incorrect.\n",
    "Opposite, overfitting is a situation when your model is too complex for your data. More formally, your hypothesis about data distribution is wrong and too complex — for example, your data is linear and your model is high-degree polynomial. This situation is also called high variance. This means that your algorithm can’t do accurate predictions — changing the input data only a little, the model output changes very much.\n",
    "Underfitting means that your model makes accurate, but initially incorrect predictions. In this case, train error is large and val/test error is large too.\n",
    "Overfitting means that your model makes not accurate predictions. In this case, train error is very small and val/test error is large.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
